{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f97ed79-3f16-4d00-90b9-5fb80d9b4ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LambdaGrader Before File Code\n",
    "# REMOVE_IN_HTML_OUTPUT\n",
    "import traceback\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "grading_start_time = datetime.now()\n",
    "\n",
    "_graded_result = {\n",
    "    'learner_score': 0,\n",
    "    'total_available': 0,\n",
    "    'results': []\n",
    "}\n",
    "\n",
    "is_lambdagrader_env = True\n",
    "\n",
    "def _record_test_case(test_case_name, did_pass, available_points, message=''):\n",
    "    global _graded_result\n",
    "    warning_message = ''\n",
    "    \n",
    "    if test_case_name in map(lambda x: x['test_case_name'], _graded_result['results']):\n",
    "        warning_message = f'[Warning] LambdaGrader: A duplicate test case name for {test_case_name} exists, duplicate items will be graded separately as well\\n\\n'\n",
    "\n",
    "    _graded_result['results'].append({\n",
    "        'test_case_name': test_case_name,\n",
    "        'available_points': available_points,\n",
    "        'points': available_points if did_pass else 0,\n",
    "        'pass': did_pass,\n",
    "        'message': warning_message + message,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b1017fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import sqlite3\n",
    "import plotly.express as px\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4ae8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = unittest.TestCase()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7749fa6e",
   "metadata": {},
   "source": [
    "## Test 1: Correct test case without a dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a6654c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "b = 2\n",
    "\n",
    "# YOUR CODE BEGINS\n",
    "c = a + b\n",
    "# YOUR CODE ENDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6682a0d2",
   "metadata": {},
   "source": [
    "### Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a21825-d153-42b4-8e18-3bba763d1063",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _test_case = 'tc-01'\n",
    "    _points = 5\n",
    "    _did_pass = True\n",
    "    _message = ''\n",
    "    \n",
    "    tc.assertEqual(c, 3)\n",
    "except BaseException as ex:\n",
    "    _did_pass = False\n",
    "    _message = ''.join(traceback.TracebackException.from_exception(ex).format())\n",
    "finally:\n",
    "    _record_test_case(_test_case, _did_pass, _points, _message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294249e7",
   "metadata": {},
   "source": [
    "## Test 2: Incorrect test case without a dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc864d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 10.5\n",
    "e = 2\n",
    "\n",
    "# YOUR CODE BEGINS\n",
    "f = d // e\n",
    "# YOUR CODE ENDS\n",
    "\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e065bf",
   "metadata": {},
   "source": [
    "### Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa78c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_case = 'tc-02'\n",
    "_points = 10\n",
    "\n",
    "_did_pass = True\n",
    "_message = \"\"\n",
    "\n",
    "try:\n",
    "    tc.assertEqual(f, 5.25)\n",
    "except BaseException as ex:\n",
    "    _did_pass = False\n",
    "    _message = \"\".join(traceback.TracebackException.from_exception(ex).format())\n",
    "finally:\n",
    "    _record_test_case(_test_case, _did_pass, _points, _message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b926482-ce29-4f32-8741-e87ade18f61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learner_score': 5, 'total_available': 15, 'results': [{'test_case_name': 'tc-01', 'available_points': 5, 'points': 5, 'pass': True, 'message': ''}, {'test_case_name': 'tc-02', 'available_points': 10, 'points': 0, 'pass': False, 'message': 'Traceback (most recent call last):\\n  File \"/tmp/ipykernel_268575/4021572123.py\", line 8, in <cell line: 7>\\n    tc.assertEqual(f, 5.25)\\n  File \"/opt/tljh/user/lib/python3.9/unittest/case.py\", line 829, in assertEqual\\n    assertion_func(first, second, msg=msg)\\n  File \"/opt/tljh/user/lib/python3.9/unittest/case.py\", line 822, in _baseAssertEqual\\n    raise self.failureException(msg)\\nAssertionError: 5.0 != 5.25\\n'}], 'graded_time': '2022-09-10T11:10:06.746503', 'grading_duration_in_seconds': 0, 'num_test_cases': 2, 'num_passed_cases': 1, 'num_failed_cases': 1}\n"
     ]
    }
   ],
   "source": [
    "# LambdaGrader After File Code\n",
    "# REMOVE_IN_HTML_OUTPUT\n",
    "import os\n",
    "\n",
    "grader_output_file_name = 'lambdagrader-result.json'\n",
    "grading_end_time = datetime.now()\n",
    "\n",
    "# Recalculate total_points and total_available points\n",
    "_graded_result['graded_time'] = grading_end_time.isoformat()\n",
    "_graded_result['grading_duration_in_seconds'] = int((grading_end_time - grading_start_time).total_seconds())\n",
    "_graded_result['learner_score'] = 0\n",
    "_graded_result['total_available'] = 0\n",
    "_graded_result['num_test_cases'] = len(_graded_result['results'])\n",
    "_graded_result['num_passed_cases'] = 0\n",
    "_graded_result['num_failed_cases'] = 0\n",
    "\n",
    "for test_case_result in _graded_result['results']:\n",
    "    _graded_result['learner_score'] += test_case_result['points']\n",
    "    _graded_result['total_available'] += test_case_result['available_points']\n",
    "    \n",
    "    if test_case_result['pass']:\n",
    "        _graded_result['num_passed_cases'] += 1\n",
    "    else:\n",
    "        _graded_result['num_failed_cases'] += 1\n",
    "\n",
    "print(_graded_result)\n",
    "    \n",
    "with open(grader_output_file_name, 'w') as fp:\n",
    "    json.dump(_graded_result, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
